
'''Ernesto Acosta - Rafael Lazcano - Sequential nets, Final Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZLTTjNZ0WCVcTmeaFdHnK7KFpBKaH3iK

**Alpha Vantage (https://www.alphavantage.co)**

**API KEY:** J41D51QYZT0JLZ99

(installation:  pip install alpha_vantage)
'''



"""## **LIBRARIES USED**
- Sklearn
- Tensorflow-Keras
- Numpy, Pandas
- Random, Os, Time Collections
- AlphaVantage
"""

import numpy as np
import pandas as pd

import random
import os
from collections import deque
from sklearn import preprocessing
import time
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense, Dropout, LSTM, CuDNNLSTM, BatchNormalization
from keras.callbacks import TensorBoard
from keras.callbacks import ModelCheckpoint
from keras.optimizers import Adam
from alpha_vantage.cryptocurrencies import CryptoCurrencies
import matplotlib.pyplot as plt

class Bitcoin_predictor_pipeline:
    def __init__(self, past_length = 40, future_distance = 2):
        self.past_length = past_length
        self.future_distance = future_distance
        self.data = None
        self.train_x = None
        self.test_x = None
        self.train_y = None
        self.test_y = None
        self.model = None
    def download_data(self, criptocoin = 'BTC', currency = 'EUR'):
        API_KEY = "J41D51QYZT0JLZ99"  # . Alpha Vantage API
        cc = CryptoCurrencies(key=API_KEY, output_format='pandas')
        data, meta_data = cc.get_digital_currency_daily(symbol=criptocoin, market=currency)

        if (currency == 'EUR'):
            data = data.drop(columns=['1b. open (USD)',
                                      '2b. high (USD)',
                                      '3b. low (USD)',
                                      '4b. close (USD)',
                                      '6. market cap (USD)'])

        data['date'] = data.index

        data = data.rename(columns={"3a. low (EUR)": "low",
                                    "2a. high (EUR)": "high",
                                    "1a. open (EUR)": "open",
                                    "4a. close (EUR)": "close",
                                    "5. volume": "volume"})

        self.data = data
    def preprocess_basic(self):
        # Set time as index so we can join them on this shared time
        self.data.set_index("date", inplace=True)

        # Keep only the "close" and "volume" columns
        self.data = self.data[["close", "volume"]]

        # Fill NaN's
        self.data.fillna(method="ffill", inplace=True)
        # TODO why drop is necessary if we already filled nans?
        self.data.dropna(inplace=True)

        # Add to each row the future value of closure
        self.data['future'] = self.data['close'].shift(-self.future_distance)
        # Drop last "PERIOD" rows because now they have nan values.
        self.data.dropna(inplace=True)

        ## here, split away some slice of the future data from the main main_self.data.
        times = sorted(self.data.index.values)
        last_records = sorted(self.data.index.values)[-int(0.1 * len(times))]

        test_df = self.data[(self.data.index >= last_records)]
        train_df = self.data[(self.data.index < last_records)]

        self.train_x, self.train_y = self.preprocess_df(train_df)
        self.test_x, self.test_y = self.preprocess_df(test_df)

    def preprocess_df(self, df):

        df = self.scale(df)

        sequential_data = []  # this is a list that will CONTAIN the sequences
        prev_days = deque(
            maxlen=self.past_length)  # These will be our actual sequences. They are made with deque, which keeps the maximum length by popping out older values as new ones come in

        for i in df.values:  # iterate over the values
            prev_days.append([n for n in i[:-1]])  # store all but the target
            if len(prev_days) == self.past_length:  # make sure we have 60 sequences!
                sequential_data.append([np.array(prev_days), i[-1]])  # append those bad boys!

        random.shuffle(sequential_data)  # shuffle for good measure.

        print(np.array(sequential_data).shape)

        random.shuffle(
            sequential_data)  # another shuffle, so the model doesn't get confused with all 1 class then the other.

        X = []
        y = []

        for seq, target in sequential_data:  # going over our new sequential data
            X.append(seq)  # X is the sequences
            y.append(target)  # y is the targets/labels (buys vs sell/notbuy)

        print(X[0])
        print(y[0])
        return np.array(X), y  # return X and y...and make X a numpy array!


    def scale(self, df):
      for col in df.columns:  # go through all of the columns
                df[col] = preprocessing.scale(df[col].values)  # scale between 0 and 1.
      return df

    def build_model(self):
        model = Sequential()
        model.add(LSTM(128, input_shape=(pipeline.train_x.shape[1:]), return_sequences=True))
        model.add(Dropout(0.2))
        model.add(BatchNormalization())  # normalizes activation outputs

        model.add(LSTM(128, return_sequences=True))
        model.add(Dropout(0.1))
        model.add(BatchNormalization())

        model.add(LSTM(128))
        model.add(Dropout(0.2))
        model.add(BatchNormalization())

        model.add(Dense(32, activation='relu'))
        model.add(Dropout(0.2))

        model.add(Dense(1, activation='linear'))
        # Model compile settings:

        opt = Adam(lr=0.001, decay=1e-6)

        # Compile model
        model.compile(
            loss='mse',
            optimizer=opt,
            metrics=['mape']
        )

        model.summary()
        self.model = model
    def train_model(self, batch_size = 64, epochs = 5):
        NAME = f"{self.past_length}-SEQ-{self.future_distance}-PRED-{int(time.time())}"
        tensorboard = TensorBoard(log_dir="logs/{}".format(NAME))
        filepath = "RNN_Final-{epoch:02d}-{mse:.3f}"
        #checkpoint = ModelCheckpoint(
        #    "models/{}.model".format(filepath, monitor='mse', verbose=1, save_best_only=True,
        #                             mode='max'))  # saves only the best ones
        history = self.model.fit(
            self.train_x, self.train_y,
            batch_size=batch_size,
            epochs=epochs,
            validation_data=(self.test_x, self.test_y),
            callbacks=[tensorboard])
        return history

    def evaluate_visualize_model(self, history):
        score = self.model.evaluate(self.test_x, self.test_y, verbose=0)

        print('Validation mean_absolute_percentage_error:', score[1])
        print('Validation mse:', score[0])

        """## **See Metrics**

        Using Pyplot, the following is displayed:
        - Accuracy on both train and validation sets
        - Moss on train and validation sets
        """

        # Loss per epoch
        plt.plot(history.history['loss'])
        plt.plot(history.history['val_loss'])
        plt.title('Loss per epoch')
        plt.ylabel('Loss')
        plt.xlabel('Epoch')
        plt.legend(['Train', 'Validation'], loc='upper left')
        plt.show()


pipeline = Bitcoin_predictor_pipeline(40,2)
pipeline.download_data()
pipeline.preprocess_basic()
print("Training records: %s" % len(pipeline.train_x))
print("Validation records: %s" % len(pipeline.test_x))

pipeline.build_model()
history = pipeline.train_model()
pipeline.evaluate_visualize_model(history)






